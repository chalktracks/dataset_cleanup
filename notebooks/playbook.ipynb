{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb44eb6c-bab2-4d21-8e18-ad18fe62ada9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from common_tools import plt_show\n",
    "root_folder = Path(\"/home/rahul/chalk_following_toy_neural_network_training/dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a23c071-f5d9-4912-9d91-75e42a8928cb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Dataset type conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4c14b61-c3a5-4b4c-98a0-a4e185445553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "images_path = root_folder / \"images\"\n",
    "label_path = root_folder / \"segmentations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56430b12-f20b-4eff-9fdb-398cbdd3a3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edging(\n",
    "    img: np.ndarray\n",
    ") -> np.ndarray:\n",
    "\n",
    "    \"\"\"Given an grayscale label image returns normalised contour\n",
    "    Taken from here: https://github.com/orgs/ultralytics/discussions/8528#discussioncomment-8868637 \"\"\"\n",
    "\n",
    "    # Dilate the labels\n",
    "    dilation_shape = cv2.MORPH_ELLIPSE\n",
    "    dilation_size = 5\n",
    "    element = cv2.getStructuringElement(dilation_shape, (2 * dilation_size + 1, 2 * dilation_size + 1),\n",
    "                                        (dilation_size, dilation_size))\n",
    "    dilated = cv2.dilate(img, element)\n",
    "    contours, _ = cv2.findContours(dilated, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if len(contours) == 0:\n",
    "        return []\n",
    "\n",
    "    # Now, select the contour with max area or iterate through all contours\n",
    "    # For example:\n",
    "    contour = max(contours, key=cv2.contourArea)\n",
    "    \n",
    "    # Simplify contour\n",
    "    epsilon = 0.001 * cv2.arcLength(contour, True)\n",
    "    approx = cv2.approxPolyDP(contour, epsilon, True)\n",
    "    \n",
    "    # Convert to normalized coordinates\n",
    "    height, width = img.shape\n",
    "    normalized_contour = approx.reshape(-1, 2) / [width, height]\n",
    "\n",
    "    return normalized_contour\n",
    "\n",
    "def contour_to_str(contour):\n",
    "\n",
    "        contour = contour.squeeze()\n",
    "        class_index = 0\n",
    "        return f\"{class_index} \" + \" \".join([f\"{x} {y}\" for x,y in contour])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67a37c8-a20d-484c-9c45-aeee957f00e7",
   "metadata": {},
   "source": [
    "***Create an new folder with Ultralytics YOLO format:***\n",
    "\n",
    "Taken from [here](https://docs.ultralytics.com/datasets/segment/)\n",
    "The dataset label format used for training YOLO segmentation models is as follows:\n",
    "\n",
    "One text file per image: Each image in the dataset has a corresponding text file with the same name as the image file and the \".txt\" extension.\n",
    "One row per object: Each row in the text file corresponds to one object instance in the image.\n",
    "Object information per row: Each row contains the following information about the object instance:\n",
    "Object class index: An integer representing the class of the object (e.g., 0 for person, 1 for car, etc.).\n",
    "Object bounding coordinates: The bounding coordinates around the mask area, normalized to be between 0 and 1.\n",
    "The format for a single row in the segmentation dataset file is as follows:\n",
    "\n",
    "\n",
    "<class-index> <x1> <y1> <x2> <y2> ... <xn> <yn>\n",
    "In this format, <class-index> is the index of the class for the object, and <x1> <y1> <x2> <y2> ... <xn> <yn> are the bounding coordinates of the object's segmentation mask. The coordinates are separated by spaces.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb0439c3-5d34-487a-b526-a967cd0254aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_dataset_dir = root_folder / \"yolo_format\" / \"temp\" / \"chalk\"\n",
    "yolo_dataset_dir.mkdir(parents = True, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e78bf336-4fef-4e6f-9564-98b1b07f5db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate the dir with appropriate text\n",
    "\n",
    "for lbl_path in label_path.rglob(\"*.png\"):\n",
    "\n",
    "    label_img = cv2.imread(\n",
    "        str(lbl_path)\n",
    "    )\n",
    "    contour = edging(label_img[:,:,2])\n",
    "    with open(yolo_dataset_dir / ( lbl_path.stem+ \".txt\" ), \"w\") as file:\n",
    "\n",
    "        if len(contour) == 0:\n",
    "            file.write(\"\")\n",
    "        else:\n",
    "            file.write(\n",
    "                contour_to_str(\n",
    "                    contour\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7aca9474-9b33-4c39-96e6-0dc2660f544c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the images from the images dir & paste it in yolo_dataset directory\n",
    "\n",
    "for img_path in images_path.iterdir():\n",
    "    shutil.copy(img_path, yolo_dataset_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0de84e1-bb98-417b-9a59-e293e4844cdf",
   "metadata": {},
   "source": [
    "YAML file will be created in next section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84872d6b-f85f-445e-bd7f-6a3b35c205fd",
   "metadata": {},
   "source": [
    "# Train Test Val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "929c7ad7-dfb9-4319-a73d-fd920337fa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path for dataset generated equivalent to yolo format\n",
    "yolo_dataset_dir = Path('/home/rahul/chalk_following_toy_neural_network_training/dataset/yolo_format/temp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4d6d128-9c3c-4d12-a2a7-afb79103e30a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 1944 files [00:00, 2867.76 files/s]\n"
     ]
    }
   ],
   "source": [
    "import splitfolders\n",
    "\n",
    "\n",
    "# Split with a ratio.\n",
    "# To only split into training and validation set, set a tuple to `ratio`, i.e, `(.8, .2)`.\n",
    "splitfolders.ratio(\n",
    "    input= yolo_dataset_dir,\n",
    "    output=yolo_dataset_dir.parent / \"dataset\",\n",
    "    seed=1337,\n",
    "    ratio=(.8, .1, .1),\n",
    "    group_prefix=None,\n",
    "    move=False) # default values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
