{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "XRumZcdfeYF_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24983,
     "status": "ok",
     "timestamp": 1721705820457,
     "user": {
      "displayName": "Rahul Mishra",
      "userId": "09090382414796014844"
     },
     "user_tz": -60
    },
    "id": "XRumZcdfeYF_",
    "outputId": "a15c059d-e8d5-4d94-afc9-ebb5c430ee45"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from inference import get_model\n",
    "import supervision as sv\n",
    "from pathlib import Path\n",
    "import os\n",
    "from retinaface import RetinaFace\n",
    "from common_tools import plt_show\n",
    "\n",
    "# dataset images directory\n",
    "root_dir = Path(\"/home/rahul/chalk_following_toy_neural_network_training/dataset/images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wX6TVfZDeVP-",
   "metadata": {
    "id": "wX6TVfZDeVP-",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Blur using Supervision\n",
    "\n",
    "This section uses Supervision to blur faces using YoloV8 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "qbMVEvQsk6X3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 824,
     "status": "ok",
     "timestamp": 1721699446199,
     "user": {
      "displayName": "Rahul Mishra",
      "userId": "09090382414796014844"
     },
     "user_tz": -60
    },
    "id": "qbMVEvQsk6X3",
    "outputId": "ba93dc33-e86f-485e-c337-a160f205fdc4"
   },
   "outputs": [],
   "source": [
    "root_dir = Path(\"/home/rahul/chalk_following_toy_neural_network_training/dataset/images\")\n",
    "model = get_model(model_id=\"yolov8n-640\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "MsM_xfDGN4Uq",
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1721699096112,
     "user": {
      "displayName": "Rahul Mishra",
      "userId": "09090382414796014844"
     },
     "user_tz": -60
    },
    "id": "MsM_xfDGN4Uq"
   },
   "outputs": [],
   "source": [
    "def pixelate_image(detection_class, image):\n",
    "  pixel_annotator = sv.PixelateAnnotator()\n",
    "  annotated_image = pixel_annotator.annotate(\n",
    "      scene = image.copy(),\n",
    "      detections=detection_class\n",
    "  )\n",
    "  return annotated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "_6-RpKeBZ5Rb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0,
     "output_embedded_package_id": "1XaE6TMjxT5yd8lXhux5FqCbbK0qw2_rc"
    },
    "executionInfo": {
     "elapsed": 6970,
     "status": "ok",
     "timestamp": 1721699945323,
     "user": {
      "displayName": "Rahul Mishra",
      "userId": "09090382414796014844"
     },
     "user_tz": -60
    },
    "id": "_6-RpKeBZ5Rb",
    "outputId": "6c7a1b29-9e00-410d-f78c-e82c2b266724"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Output hidden; open in https://colab.research.google.com to view."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for image_path in root_dir.rglob(\"*.png\"):\n",
    "\n",
    "  image = cv2.imread(str(image_path))\n",
    "  result = model.infer(image)[0]\n",
    "  detection = sv.Detections.from_inference(result)\n",
    "  annotated_img = pixelate_image(detection, image)\n",
    "  plt_show(annotated_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hNxt5t47e8wD",
   "metadata": {
    "id": "hNxt5t47e8wD",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Blur using retina face\n",
    "\n",
    "This section uses [retinaface](https://github.com/serengil/retinaface) to blur faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "RmO0zs0RXEFg",
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1721705820459,
     "user": {
      "displayName": "Rahul Mishra",
      "userId": "09090382414796014844"
     },
     "user_tz": -60
    },
    "id": "RmO0zs0RXEFg"
   },
   "outputs": [],
   "source": [
    "def pixelate_face(image:np.ndarray, blocks:int=10) ->np.ndarray:\n",
    "\n",
    "  \"\"\"Taken from here: https://github.com/h0ssn1/blur-face/tree/main\"\"\"\n",
    "  # divide the input image into NxN blocks\n",
    "  (h, w) = image.shape[:2]\n",
    "  xSteps = np.linspace(0, w, blocks + 1, dtype=\"int\")\n",
    "  ySteps = np.linspace(0, h, blocks + 1, dtype=\"int\")\n",
    "  # loop over the blocks in both the x and y direction\n",
    "  for i in range(1, len(ySteps)):\n",
    "      for j in range(1, len(xSteps)):\n",
    "          # compute the starting and ending (x, y)-coordinates\n",
    "          # for the current block\n",
    "          startX = xSteps[j - 1]\n",
    "          startY = ySteps[i - 1]\n",
    "          endX = xSteps[j]\n",
    "          endY = ySteps[i]\n",
    "          # extract the ROI using NumPy array slicing, compute the\n",
    "          # mean of the ROI, and then draw a rectangle with the\n",
    "          # mean RGB values over the ROI in the original image\n",
    "          roi = image[startY:endY, startX:endX]\n",
    "          (B, G, R) = [int(x) for x in cv2.mean(roi)[:3]]\n",
    "          cv2.rectangle(image, (startX, startY), (endX, endY),\n",
    "                        (B, G, R), -1)\n",
    "  # return the pixelated blurred image\n",
    "  return image\n",
    "\n",
    "def blur(img: np.ndarray, k: int) -> np.ndarray:\n",
    "    \"\"\"Taken from here: https://github.com/h0ssn1/blur-face/tree/main\"\"\"\n",
    "    h, w = img.shape[:2]\n",
    "    print(f\"img shape: {img.shape}\")\n",
    "    kh, kw = h // k, w // k\n",
    "    if kh % 2 == 0:\n",
    "      kh -= 1\n",
    "    if kw % 2 == 0:\n",
    "      kw -= 1\n",
    "    print(f\"kh: {kh}, kw: {kw}\")\n",
    "    img = cv2.GaussianBlur(img, ksize=(kh, kw), sigmaX=0)\n",
    "    return img\n",
    "\n",
    "def blur_face(image: np.ndarray) -> np.ndarray:\n",
    "  \"\"\"Uses the above 2 function & retinaface to blur faces\"\"\"\n",
    "  faces = RetinaFace.detect_faces(image)\n",
    "\n",
    "  if not faces:\n",
    "    return image\n",
    "  else:\n",
    "    for face in faces.values():\n",
    "        x1,y1,x2,y2 = face['facial_area']\n",
    "\n",
    "        image[y1:y2, x1:x2] = pixelate_face(blur(image[y1:y2, x1:x2], 2))\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Fcy3L2vlmETA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1x-VyDI8SVGmOqDSKQtdiogWq1WiCzeGT"
    },
    "executionInfo": {
     "elapsed": 182047,
     "status": "ok",
     "timestamp": 1721706079766,
     "user": {
      "displayName": "Rahul Mishra",
      "userId": "09090382414796014844"
     },
     "user_tz": -60
    },
    "id": "Fcy3L2vlmETA",
    "outputId": "e23555a5-3e79-4cad-9ede-0cf8a63c5298",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_dir = root_dir.parent / Path(\"blured_images\")\n",
    "result_dir.mkdir(exist_ok=True)\n",
    "for image_path in root_dir.rglob(\"*.png\"):\n",
    "\n",
    "    print(f\" img_path: {image_path}\")\n",
    "    image = cv2.imread(str(image_path))\n",
    "    image = blur_face(image)\n",
    "    ret = cv2.imwrite(\n",
    "        str(\n",
    "            result_dir/ image_path.name\n",
    "        ),\n",
    "        image\n",
    "    )\n",
    "    if ret == False: raise FileExistsError\n",
    "    plt_show(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fcb0f0-ae05-4b9a-b42b-823c8fb61254",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Dataset type conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cc16d0-0c8c-4f8e-8734-254e45d0f25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from common_tools import plt_show\n",
    "root_folder = Path(\"/home/rahul/chalk_following_toy_neural_network_training/dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05848285-4748-492c-8360-6acc8bcbe454",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "images_path = root_folder / \"images\"\n",
    "label_path = root_folder / \"segmentations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e709c4-129b-49c0-9a38-d937bacc64ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edging(\n",
    "    img: np.ndarray\n",
    ") -> np.ndarray:\n",
    "\n",
    "    \"\"\"Given an grayscale label image returns normalised contour\n",
    "    Taken from here: https://github.com/orgs/ultralytics/discussions/8528#discussioncomment-8868637 \"\"\"\n",
    "\n",
    "    # Dilate the labels\n",
    "    dilation_shape = cv2.MORPH_ELLIPSE\n",
    "    dilation_size = 5\n",
    "    element = cv2.getStructuringElement(dilation_shape, (2 * dilation_size + 1, 2 * dilation_size + 1),\n",
    "                                        (dilation_size, dilation_size))\n",
    "    dilated = cv2.dilate(img, element)\n",
    "    contours, _ = cv2.findContours(dilated, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if len(contours) == 0:\n",
    "        return []\n",
    "\n",
    "    # Now, select the contour with max area or iterate through all contours\n",
    "    # For example:\n",
    "    contour = max(contours, key=cv2.contourArea)\n",
    "    \n",
    "    # Simplify contour\n",
    "    epsilon = 0.001 * cv2.arcLength(contour, True)\n",
    "    approx = cv2.approxPolyDP(contour, epsilon, True)\n",
    "    \n",
    "    # Convert to normalized coordinates\n",
    "    height, width = img.shape\n",
    "    normalized_contour = approx.reshape(-1, 2) / [width, height]\n",
    "\n",
    "    return normalized_contour\n",
    "\n",
    "def contour_to_str(contour):\n",
    "\n",
    "        contour = contour.squeeze()\n",
    "        class_index = 0\n",
    "        return f\"{class_index} \" + \" \".join([f\"{x} {y}\" for x,y in contour])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f50a81a-e1a4-4a3d-9388-8ef48c8582df",
   "metadata": {},
   "source": [
    "***Create an new folder with Ultralytics YOLO format:***\n",
    "\n",
    "Taken from [here](https://docs.ultralytics.com/datasets/segment/)\n",
    "The dataset label format used for training YOLO segmentation models is as follows:\n",
    "\n",
    "One text file per image: Each image in the dataset has a corresponding text file with the same name as the image file and the \".txt\" extension.\n",
    "One row per object: Each row in the text file corresponds to one object instance in the image.\n",
    "Object information per row: Each row contains the following information about the object instance:\n",
    "Object class index: An integer representing the class of the object (e.g., 0 for person, 1 for car, etc.).\n",
    "Object bounding coordinates: The bounding coordinates around the mask area, normalized to be between 0 and 1.\n",
    "The format for a single row in the segmentation dataset file is as follows:\n",
    "\n",
    "\n",
    "<class-index> <x1> <y1> <x2> <y2> ... <xn> <yn>\n",
    "In this format, <class-index> is the index of the class for the object, and <x1> <y1> <x2> <y2> ... <xn> <yn> are the bounding coordinates of the object's segmentation mask. The coordinates are separated by spaces.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6b9e1c-ca04-4ea9-aeb4-8c368dcc09bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_dataset_dir = root_folder / \"yolo_format\" / \"temp\" / \"chalk\"\n",
    "yolo_dataset_dir.mkdir(parents = True, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e4479e-a95b-43dc-bc82-97b233cc1294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate the dir with appropriate text\n",
    "\n",
    "for lbl_path in label_path.rglob(\"*.png\"):\n",
    "\n",
    "    label_img = cv2.imread(\n",
    "        str(lbl_path)\n",
    "    )\n",
    "    contour = edging(label_img[:,:,2])\n",
    "    with open(yolo_dataset_dir / ( lbl_path.stem+ \".txt\" ), \"w\") as file:\n",
    "\n",
    "        if len(contour) == 0:\n",
    "            file.write(\"\")\n",
    "        else:\n",
    "            file.write(\n",
    "                contour_to_str(\n",
    "                    contour\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6946d99-2931-4513-a41c-36b91cdb03a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the images from the images dir & paste it in yolo_dataset directory\n",
    "\n",
    "for img_path in images_path.iterdir():\n",
    "    shutil.copy(img_path, yolo_dataset_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8ec331-bfdb-44e6-8159-6976799f2d58",
   "metadata": {},
   "source": [
    "YAML file will be created in next section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e560d6-5bf8-400e-9a08-09def31d5e1b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Train Test Val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0541f015-7c17-4970-b53b-04fc2b375eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path for dataset generated equivalent to yolo format\n",
    "yolo_dataset_dir = Path('/home/rahul/chalk_following_toy_neural_network_training/dataset/yolo_format/temp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61812a17-cae8-4f09-b453-eab312eb96d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import splitfolders\n",
    "\n",
    "\n",
    "# Split with a ratio.\n",
    "# To only split into training and validation set, set a tuple to `ratio`, i.e, `(.8, .2)`.\n",
    "splitfolders.ratio(\n",
    "    input= yolo_dataset_dir,\n",
    "    output=yolo_dataset_dir.parent / \"dataset\",\n",
    "    seed=1337,\n",
    "    ratio=(.8, .1, .1),\n",
    "    group_prefix=None,\n",
    "    move=False) # default values"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "pPVWMQLDVRRr",
    "wX6TVfZDeVP-"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
